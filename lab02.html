<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Evaluation | Chi Hoang</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="lab02.html" class="active">Lab 2: AI Evaluation</a>
            <!-- Add more lab links as the semester progresses -->
        </nav>
        <h1>AI Tool Evaluation</h1>
    </header>

    <main>
        <section>
            <h2>Introduction</h2>
            <p>I’m looking at DeepSeek and Consensus through a DCDA lens. Using astrology and resume optimization as tests to shed light on different AI’s capabilities, appropriate use and ethical nuances.</p>
        </section>

        <section>
            <h2>Tool 1: Deepseek</h2>
            <h3>Capabilities</h3>
            <p>I really appreciate how DeepSeek makes its thinking process explicit; seeing the "Chain-of-Thought" laid out helps me understand exactly what the algorithm is focusing on to generate an output. This transparency makes me feel more in control of the interaction. I also find the product design, specifically the truncated scroll bar, incredibly convenient for revisiting my previous chats without endless scrolling. However, I’ve found it falls short when it comes to math and cross-disciplinary tasks. It seems to struggle when I try to connect disparate fields, often providing shallow answers that don’t quite bridge the gap between complex subjects effectively.
</p>

            <h3>Appropriate Use</h3>
            <p>I believe DeepSeek works best for tasks that require deep focus on a specific, self-contained set of knowledge. For example, I found it surprisingly effective for astrology queries and resume making because its answers are so specific and tailored to those niche formats. It feels like a specialized assistant for structured content. However, I’ve found it to be a poor choice for math or anything requiring high-level reasoning. </p>

            <h3>Ethical Considerations</h3>
            <p>During my exploration, I encountered several ethical and performance frustrations. The tool is often quite slow, and I’ve had it randomly switch to Chinese or refuse to answer entirely because the server was "too busy." Beyond that, I find the model lacks objectivity; it tends to adorn its responses with unnecessary adjectives. This makes me worry about a lack of focus on neutrality. It feels less like a neutral assistant and more like it’s pushing a specific tone, which can be misleading when I am looking for unbiased, objective information.
</p>

            <!-- Add screenshots or examples -->
            <figure>
                <img src="images/deepseek.png" alt="deepseek answers in Chinese, with scrollbar visible">
                <figcaption>In this screenshot, DeepSeek answered an English prompt about resume in Chinese. The screenshot also featured the scrollable sidebar with chat history shown.</figcaption>
            </figure>
        </section>

        <section>
             <h2>Tool 2: Consensus</h2>
            <h3>Capabilities</h3>
            <p>I found that Consensus excels at finding citations and visualizing prompts for simple yes/no queries. I really value the "Consensus Meter" because it gives me a clear snapshot of scientific agreement, and the clickable, verified citations make me feel much more confident in the data. However, I found it falls short when I’m working on creative or non-supported claims; it just doesn’t have the reasoning power to help with an original thesis. While the product design is a bit cluttered with a "sprawl" of information for each output, it remains clear enough to be useful for high-level research.
</p>
            <h3>Appropriate Use</h3>
           <p>In my experience, Consensus is a fantastic tool for literature reviews, as it allows me to quickly aggregate what the academic community says about a specific topic. It really streamlines the early stages of research and fact-checking. However, I’ve found it to be a poor use case for tasks that require deep reasoning or creative brainstorming. If I am trying to develop an original argument or a non-traditional thesis, the tool struggles because it is strictly bound to existing literature. 
</p>
            <h3>Ethical Considerations</h3>
            <p> One of my primary concerns with Consensus is the potential for bias toward major journals, which might end up excluding dissenting minority views in the scientific community. It risks creating a "majority-rule" approach to truth. I also find it problematic that the tool is so heavily concentrated in STEM and social sciences. It is harder to find nuanced points for the humanities discipline.</p>
        <figure>
                <img src="images/consensus.png" alt="Answer of Consensus">
                <figcaption>In this screenshot, Consense answered a question prompt instead of a statement prompt. The answer content is long and sprawling as always, yet the stance that astrology is not real is clear. However, i think as "is astrology real?" is a question, Consensus should have built a case to prove that it was wrong, instead of just using the number game without acknowledging limitations.</figcaption>
            </figure>
        </section>

        <section>
            <h2>Broader Reflections</h2>
            <p><strong>Personal Use:</strong> In my personal workflow, I enjoy brainstorming and laying out the initial architecture of an idea myself. I then use AI as an editor to smooth out my phrasing, back up my claims with evidence, and refine my established ideas. This collaborative approach allows me to maintain creative control while using the AI to support the final output. 
<br><strong>Context:</strong> For coding specifically, I find AI really helps in getting the big scary first steps out of the way. When I have a concept, I describe my needs to the AI to generate initial, albeit crude, code. I’ve found that these outputs are rarely detailed enough on their own, so I take over the refinement process, using pseudo-code to fix logic gaps and enhance specificity. This hybrid method allows me to create highly customized, complex projects. For the Digital Humanities aspect, I primarily rely on AI for fact-checking, validating my theories, and helping me find the right articulation for arguments.
<br><strong>Evolving Landscape:</strong> I think using astrology is a great way to test AI. Astrology has some concrete falsifiable rules systems that need to be interpreted. I can see AI adjective use rate, its logical reasoning, and its tendency to overlook, or look at the subject methodologically and holistically, and if its answer and interpretation is logic based or narrative based. I can even see if the AI is people pleasing, or able to say no and provide objective truth.  
</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2026 Chi Hoang | <a href="https://github.com/chihm12/DCDA40833-portfolio-main">GitHub</a></p>
    </footer>
</body>
</html>